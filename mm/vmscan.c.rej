--- mm/vmscan.c
+++ mm/vmscan.c
@@ -368,6 +372,7 @@ out:
 	return ret;
 }
 
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 static void set_reclaim_mode(int priority, struct scan_control *sc,
 				   bool sync)
 {
@@ -440,9 +464,11 @@ static int may_write_to_queue(struct backing_dev_info *bdi,
 	if (bdi == current->backing_dev_info)
 		return 1;
 
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 	/* lumpy reclaim for hugepage often need a lot of write */
 	if (sc->order > PAGE_ALLOC_COSTLY_ORDER)
 		return 1;
+#endif
 	return 0;
 }
 
@@ -733,9 +759,11 @@ static enum page_references page_check_references(struct page *page,
 	referenced_ptes = page_referenced(page, 1, mz->mem_cgroup, &vm_flags);
 	referenced_page = TestClearPageReferenced(page);
 
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 	/* Lumpy reclaim - ignore references */
 	if (sc->reclaim_mode & RECLAIM_MODE_LUMPYRECLAIM)
 		return PAGEREF_RECLAIM;
+#endif
 
 	/*
 	 * Mlock lost the isolation race with us.  Let try_to_unmap()
@@ -847,7 +875,11 @@ static unsigned long shrink_page_list(struct list_head *page_list,
 				wait_on_page_writeback(page);
 			else {
 				unlock_page(page);
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 				goto keep_lumpy;
+#else
+				goto keep_reclaim_mode;
+#endif
 			}
 		}
 
@@ -931,7 +963,11 @@ static unsigned long shrink_page_list(struct list_head *page_list,
 				goto activate_locked;
 			case PAGE_SUCCESS:
 				if (PageWriteback(page))
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 					goto keep_lumpy;
+#else
+					goto keep_reclaim_mode;
+#endif
 				if (PageDirty(page))
 					goto keep;
 
@@ -1031,7 +1067,11 @@ keep_locked:
 		unlock_page(page);
 keep:
 		reset_reclaim_mode(sc);
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 keep_lumpy:
+#else
+keep_reclaim_mode:
+#endif
 		list_add(&page->lru, &ret_pages);
 		VM_BUG_ON(PageLRU(page) || PageUnevictable(page));
 	}
@@ -1087,11 +1127,7 @@ int __isolate_lru_page(struct page *page, isolate_mode_t mode, int file)
 	if (!all_lru_mode && !!page_is_file_cache(page) != file)
 		return ret;
 
-	/*
-	 * When this function is being called for lumpy reclaim, we
-	 * initially look into all LRU pages, active, inactive and
-	 * unevictable; only give shrink_page_list evictable pages.
-	 */
+	/* Do not give back unevictable pages for compaction */
 	if (PageUnevictable(page))
 		return ret;
 
@@ -1176,9 +1212,11 @@ static unsigned long isolate_lru_pages(unsigned long nr_to_scan,
 	struct lruvec *lruvec;
 	struct list_head *src;
 	unsigned long nr_taken = 0;
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 	unsigned long nr_lumpy_taken = 0;
 	unsigned long nr_lumpy_dirty = 0;
 	unsigned long nr_lumpy_failed = 0;
+#endif
 	unsigned long scan;
 	int lru = LRU_BASE;
 
@@ -1191,10 +1229,12 @@ static unsigned long isolate_lru_pages(unsigned long nr_to_scan,
 
 	for (scan = 0; scan < nr_to_scan && !list_empty(src); scan++) {
 		struct page *page;
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 		unsigned long pfn;
 		unsigned long end_pfn;
 		unsigned long page_pfn;
 		int zone_id;
+#endif
 
 		page = lru_to_page(src);
 		prefetchw_prev_lru_page(page, src, flags);
@@ -1217,6 +1257,7 @@ static unsigned long isolate_lru_pages(unsigned long nr_to_scan,
 			BUG();
 		}
 
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 		if (!sc->order || !(sc->reclaim_mode & RECLAIM_MODE_LUMPYRECLAIM))
 			continue;
 
@@ -1294,6 +1335,7 @@ static unsigned long isolate_lru_pages(unsigned long nr_to_scan,
 		/* If we break out of the loop above, lumpy reclaim failed */
 		if (pfn < end_pfn)
 			nr_lumpy_failed++;
+#endif
 	}
 
 	*nr_scanned = scan;
@@ -1301,7 +1343,9 @@ static unsigned long isolate_lru_pages(unsigned long nr_to_scan,
 	trace_mm_vmscan_lru_isolate(sc->order,
 			nr_to_scan, scan,
 			nr_taken,
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 			nr_lumpy_taken, nr_lumpy_dirty, nr_lumpy_failed,
+#endif
 			mode, file);
 	return nr_taken;
 }
@@ -1495,7 +1539,7 @@ static inline bool should_reclaim_stall(unsigned long nr_taken,
 	if (current_is_kswapd())
 		return false;
 
-	/* Only stall on lumpy reclaim */
+	/* Only stall for memory compaction */
 	if (sc->reclaim_mode & RECLAIM_MODE_SINGLE)
 		return false;
 
@@ -1546,8 +1590,10 @@ shrink_inactive_list(unsigned long nr_to_scan, struct mem_cgroup_zone *mz,
 	}
 
 	set_reclaim_mode(priority, sc, false);
+#ifndef CONFIG_DISABLE_LUMPY_RECLAIM
 	if (sc->reclaim_mode & RECLAIM_MODE_LUMPYRECLAIM)
 		isolate_mode |= ISOLATE_ACTIVE;
+#endif
 
 	lru_add_drain();
 
@@ -1973,11 +2019,14 @@ static void get_scan_count(struct mem_cgroup_zone *mz, struct scan_control *sc,
 	}
 
 	/*
-	 * With swappiness at 100, anonymous and file have the same priority.
 	 * This scanning priority is essentially the inverse of IO cost.
 	 */
 	anon_prio = vmscan_swappiness(mz, sc);
+#ifdef CONFIG_ZSWAP
+	file_prio = max_swappiness - vmscan_swappiness(mz, sc);
+#else
 	file_prio = 200 - vmscan_swappiness(mz, sc);
+#endif
 
 	/*
 	 * OK, so we have swap space and a fair amount of page cache
@@ -2880,7 +2938,7 @@ loop_again:
 
 			if (!zone_watermark_ok_safe(zone, testorder,
 					high_wmark_pages(zone), end_zone, 0)) {
-				all_zones_ok = 0;
+				unbalanced_zone = zone;
 				/*
 				 * We are still under min water mark.  This
 				 * means that we have a GFP_ATOMIC allocation
